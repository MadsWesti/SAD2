\documentclass[a4paper,11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{listings}
\usepackage{mathtools}   % need for subequations
\usepackage{graphicx}   % need for figures
\usepackage{verbatim}   % useful for program listings
\usepackage{color}      % use if color is used in text
\usepackage{hyperref}  
\usepackage{url}
\usepackage{float}
\usepackage{todonotes}
\usepackage{tikz}
\usetikzlibrary{arrows}
\usepackage{enumitem}
\usepackage{hyperref}
\usepackage{pdfpages}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{listings}
\usepackage{color}
\usepackage{amsfonts}
\usepackage{latexsym}
\usepackage[T1]{fontenc} % use for allowing < and > in cleartext
\usepackage{fixltx2e}    % use for textsubscript
\usepackage[framemethod=tikz]{mdframed}

\usepackage[margin=2.5cm]{geometry}
\usepackage[ampersand]{easylist}
\usepackage{epstopdf}

\bibliographystyle{plain}

\begin{document}

\setlength{\parindent}{0cm}
\setlength{\unitlength}{1mm}
\date{December 17th 2014\\ IT University of Copenhagen}
\title{Roles In Movies\\SAD2 Fall 2014}

\author{Sarah de Voss\\
\texttt{satv@itu.dk}
\and Elvis Flesborg\\
\texttt{efle@itu.dk}
\and Mads Westi\\
\texttt{mwek@itu.dk}}
\clearpage\maketitle

\thispagestyle{empty}
\newpage
\tableofcontents
\thispagestyle{empty}
\newpage

\setcounter{page}{1}
\section{Finding a Similar Actor}

Say you want to make a movie with a specific actor as the lead role, but the actor is unavailable. You have already written the script, and found all the other actors. The only thing missing is the lead actor. \\

You find a similar actor. You find the most similar actor. And for this we must assume that, what defines an actor, is what movies it appears in. So you find the actor that has the most movies in common with your intended actor, and use that actor instead. \\

The size of a database expands every day with new entries. The IMDB (Intenet Movie Database) contains at this moment 817718 actors and 388215 movies. Checking for comparisons is labourous work even for a pc. The amount of comparisons is $O(a^2\cdot m)$ where $a$ is the number of actors and $m$ is the number of movies. This gives us $2.6\cdot 10^{17}$ comparisons!\\

For solving this problem we have created an algorithm that does similarity search through the \emph{IMDB database} and implemented different approximation schemes to make the process faster without loosing to much accuracy.


\section{Implementation}
%\begin{mdframed}[hidealllines=true,backgroundcolor=blue!20]
We would like to find a similar actor to replace our unavailable lead actor. When is an actor similar to another? We start by discussing similarity and which similarity measure we are using.\\
%\end{mdframed}

We are using Python to implement our algorithms as this is our language of choice. \\

We will look at the naïve solution, then \emph{MinHashing}, and then \emph{Locality Sensitive Hashing}, this is part one of the report. After that we will implement \emph{b-bit} and \emph{odd sketch}, which is the extension to our report. For each algorithm we will explain how it works and its application and then go on to explain how we implemented it. We then explain the time and space complexity. We finish each section by discussing the results we achieved.\\


\subsection{Measuring similarity}
Since a role is represented as a set movies, we can measure the similarity between roles using the Jaccard similarity, where the similarity between two sets $S_1$ and $S_2$.

\begin{equation}
Sim(S_1, S_2) = \frac{|S_1 \cap S_2|}{|S_1 \cup S_2|}
\end{equation}
The Jaccard index falls in the range of 0-1. It is 1 if the sets are similar, and it is 0 if the sets are disjoint.


\subsection{Measuring Running Time}
For every new implementation, we will measure the actual running time. It will be measured on a normal laptop computer with 8GB RAM and and intel i5 quadcore. Though, if the running time is taking longer than out patience permits, we will just put in how long we waited, and note that we did not run it through.

\todo{rewrite actor}
We will also create some random dummy data to run. This way we will be able to graph the running times, and look at the tendency curves. In the dummy data, there are 10 times as many roles, as there are movies. This corresponds pretty well with the \emph{imdb-r.txt} file, which has 57,736 distinct movies, and 553,263 distinct roles (not counting the movies without any roles and roles with nonexisting movie-ids).

\todo{rewrite actor}
But even though there are 10 more roles per movies. Many of these roles are only in one movie, and only few roles are in many movies. If we calculate the average number of movies, that a role appears in, it is somewhere between 1.2 and 2.0, depending on what our cut-off rank is (see figure ~\ref{fig:sparsenesscutoff}. We have defined our minimum rank cutoff to movies with at least 7.0 in rank, which corresponds to an average of 1.7 movies per role. This has influence on the characteristic matrix, that we create when parsing.

This will be taken into account when creating our random dummy data.

\begin{figure}[!htbp]
    \begin{center}
        \input{plots/ones_per_role.tex}
        \caption{Sparseness with cutoff}
        \label{fig:sparsenesscutoff}
    \end{center}
\end{figure}

\subsection{Naïve Implementation}

\subsubsection{Data representation}
An actor is represented as a set of movies\footnote{Corresponding to the ids of the movies, which are integers} the specific actor appear in.

\begin{equation}
a_1 = \{m_1, m_2, m_3, \ldots , m_i\}
\end{equation}

Instead of storing the actors as sets, for reasons that become more apparent in section \ref{sec:minhashing}, it is beneficial to view the data as a binary characteristic matrix, where the rows are movies and the columns are actors, and the values of the matrix defines whether an actor had a role in a given movie or not. 

\begin{figure}[!htbp]
\begin{eqnarray*}
 & n \ \text{roles} \\
 m \ \text{movies} = & 
\begin{cases}
    \overbrace{
    \begin{bmatrix}
        1 & 0 & 1 & 0 \\
        1 & 0 & 0 & 1 \\
        0 & 1 & 0 & 1 \\
        0 & 1 & 0 & 1 \\
        0 & 1 & 0 & 1 \\
        1 & 0 & 1 & 0 \\
        1 & 0 & 1 & 0
    \end{bmatrix} 
    }
\end{cases}
\end{eqnarray*}
\caption{Example of a characteristic matrix}
\label{fig:char_matrix}
\end{figure}

If we were to save the data as the full matrix as seen in ~\ref{fig:char_matrix}, the space complexity would be $\Theta(n \times m)$. In the actual implementation \todo{reference til koden}The characteristic matrix is stored as an dictionary to avoid storing 0's in the matrix, as suggested in \cite{book:mmds} p. 81. The space complexity is still upper bound by $(a \cdot m)$, but since the matrix in most cases is very sparse, the average space consumption is much less. In our case with the IMDB data set:
\begin{equation}
a = 460428
m = 63594
s = 1323388
ratio = \frac{1323388}{63594 \cdot 460428} = 4.52\cdot 10^{-5}
\end{equation}

The naïve solution is calculating the Jaccard similarity using the characteristic matrix $D$, by looping through all pairs of actors, and for each pair loop through all the movies. We define $x = |S_1 \cap S_2|$\footnote{i.e the number of movies for which $D$ contains an entry for booth actors}. If we view the set union as $|S_1 \cup S_2| = |S_1 \oplus S_2| + |S_1 \cap S_2|$, we can define $y = |S_1 \oplus S_2|$\footnote{i.e the symmetric difference of the sets, the number of movies for which there are an entry for either one of the actor or the other in $D$}. The Jaccard similarity can therefore be computed as:

\begin{equation}
J(S_1,S_2) = \frac{x}{x+y}
\end{equation}


Since we have three nested for loops the time complexity is ...

Because we have to permute every possible pair of roles, the running time is $O(n^2 \cdot m)$, where $n$ is the number of distinct roles, and $m$ is the number of movies. In a scenario where $m$ is almost equal to $n$, the runtime will be $O(n^3)$. \\

This will surely not fit in memory with the whole dataset, so more sofisticated algorithms are needed. \\

\subsubsection{Actual Running Time}
The actual running time with the naïve implementation takes too long for our normal laptop computer to outrun our patience. We ran the implementation on random dummy datasets, which can be seen in figure ~\ref{fig:naive_at}.

\begin{figure}[!htbp]
    \begin{center}
        \input{plots/naive_at.tex}
        \caption{Naïve Implementation Actual Times}
        \label{fig:naive_at}
    \end{center}
\end{figure}


\section{MinHashing}\label{sec:minhashing}
Since the permutation of such a large matrix is infeasible, we create a number of hash functions, $h_1, ..., h_k$, which should be less than the number of rows in the characteristic matrix. To compare sets of documents, the normal procedure is to convert them into shingles. But since the "documents" we are working on, are actual sets of index numbers, there is no need for us to use shingles. \\

Remarkably, the probability that the minhash function produces the same value for two sets is the same as the Jaccard similarity of the two sets (p. 82 in \cite{book:mmds}). Let \\

    $x = $ the rows in which the two sets both have a 1. \\
    $y = $ the rows in which one of the sets have a 1. \\
    $z = $ the rows in which none of the sets have a 1. \\

Then the probability of having found a row in $x$, for a random permutation, is

\begin{equation*}
    \frac{x}{x+y}
\end{equation*}

which is equals to the Jaccard similarity of the two sets.

\begin{equation*}
    \frac{S_1 \cap S_2}{S_1 \cup S_2} = \frac{x}{x+y}
\end{equation*}
    
We simulate $k$ random permutations by creating $k$ number of random hash functions \footnote{We are using universal hashing ($(ax + b) \bmod p$) as our hashing functions, where there will be some collisions in the hashes. Some row indexes will be hashed to the same, but as long as $k$ is large it is not important (p. 83-84 in \cite{book:mmds}).}. All row indices are then hashed with all these hash functions, to simulate a permutation. When this is done, we have created the signature matrix, which is a $k\times n$ matrix, that contains the minimum value of $h(x)$, where $x$ is an element in $S$. See figure ~\ref{fig:signature_matrix}\\

\begin{figure}[!htpb]
    \begin{eqnarray*}
     & n \ \text{signatures} \\
     k \ \text{permutations} = & 
        \begin{cases}
        \overbrace{
        \begin{bmatrix}
            1 & 1 & 4 & 3 & 3 & 6 & 4\\
            3 & 3 & 1 & 5 & 3 & 2 & 1\\
            4 & 3 & 2 & 3 & 3 & 4 & 5\\
            0 & 1 & 4 & 2 & 2 & 6 & 1\\
        \end{bmatrix} 
        } 
        \end{cases}
    \end{eqnarray*}
    \caption{Signature matrix}
    \label{fig:signature_matrix}
\end{figure}

We still have to permute all pairs of roles, but the number of comparisons is reduced to $k$, the new running time is therefore $O(n^2 \cdot k)$ compared to $O(n^2 \cdot m)$ of the naïve solution, see figure ~\ref{fig:char_matrix} versus ~\ref{fig:signature_matrix}.

\subsection{Actual Running Times}
In figure ~\ref{fig:minhashing_at} the actual running times can be seen.


\begin{figure}[!htpb]
    \begin{center}
        \input{plots/minhashing_at.tex}
        \caption{MinHashing Actual Times}
        \label{fig:minhashing_at}
    \end{center}
\begin{equation}
    \text {S-curve equation} = 1 - (1 - s^r)^b 
\end{equation}
\end{figure}


\section{Locality Sensitive Hashing (LSH)}
Locality sensitive hashing is a way of coping with an even larger amount of data than MinHashing can. So if the number of pairs is too large, LSH is an option to use. We take the signatures produced by min hashing and chop them up into shorter vectors by dividing the total number of rows into $b$ bands of $r$ rows. We then hash the vectors into a large number of buckets. If 2 vectors hash to the same bucket there is a good chance that they are similar. These vectors become candidate pairs, and we only compare candidate pairs for similarity. This leaves us with a significant fraction of the original set of pairs to check. \\

In figure \ref{fig:lsh_buckets} the signature matrix from earlier has been dissected into 2 bands. These bands have, in this example, the same number of buckets as there are signatures. The buckets containing 2 or more signatures have been marked in green. If a signature is in one or more buckets it is a candidate signature to be checked for similarity, whereas if a signature is only in red buckets, which have only one or zero signatures, it is discarded as a candidate signature. \\

So the candidate signatures are $S_0, S_1, S_3, S_4, S_6$, and the discarded signatures are $S_2, S_5$.

\begin{figure}[!htpb]
    \begin{center}
        \begin{tikzpicture}
            \draw rectangle (14,4);
            \draw rectangle (7,2);
            \draw (0,2) rectangle (7,2);
            \draw (7,0) rectangle (8,4);

            \foreach \x in {0,...,6} \draw (\x+7, 0) rectangle (\x+7+1, 2);
            \foreach \x in {0,...,6} \draw (\x+7, 2) rectangle (\x+7+1, 4);
            \foreach \x in {0,...,6} \draw (\x+.5, 4.5) node {$S_\x$};

            \draw (-.5,3) node {$b_0$};
            \draw (-.5,1) node {$b_1$};

            % The overbrace
            \draw (10.5, 4.5) node {$\overbrace{\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad}$};
            \draw (10.5,5.5) node {buckets};

            % Signature matrix (left part)
            \draw (0.5,3.5) node {1};
            \draw (0.5,2.5) node {3};
            \draw (0.5,1.5) node {4};
            \draw (0.5,0.5) node {0};

            \draw (1.5,3.5) node {1};
            \draw (1.5,2.5) node {3};
            \draw (1.5,1.5) node {3};
            \draw (1.5,0.5) node {1};

            \draw (2.5,3.5) node {4};
            \draw (2.5,2.5) node {1};
            \draw (2.5,1.5) node {2};
            \draw (2.5,0.5) node {4};

            \draw (3.5,3.5) node {3};
            \draw (3.5,2.5) node {5};
            \draw (3.5,1.5) node {3};
            \draw (3.5,0.5) node {2};

            \draw (4.5,3.5) node {3};
            \draw (4.5,2.5) node {3};
            \draw (4.5,1.5) node {3};
            \draw (4.5,0.5) node {2};

            \draw (5.5,3.5) node {6};
            \draw (5.5,2.5) node {2};
            \draw (5.5,1.5) node {4};
            \draw (5.5,0.5) node {6};

            \draw (6.5,3.5) node {1};
            \draw (6.5,2.5) node {3};
            \draw (6.5,1.5) node {3};
            \draw (6.5,0.5) node {1};

            % Fill all with red (in right part)
            \foreach \x in {0,...,6} \path[fill=red!30] (7.1+\x,0.1) rectangle (7.9+\x,1.9);
            \foreach \x in {0,...,6} \path[fill=red!30] (7.1+\x,2.1) rectangle (7.9+\x,3.9);

            % Fill the right ones with green
            \path[fill=green!50] (7.1,2.1) rectangle (7.9,3.9);
            \path[fill=green!50] (8.1,0.1) rectangle (8.9,1.9);
            \path[fill=green!50] (10.1,0.1) rectangle (10.9,1.9);

            % Names
            \draw (7.5,3.5) node {$S_0$};
            \draw (7.5,3.0) node {$S_1$};
            \draw (7.5,2.5) node {$S_6$};
            \draw (8.5,3.5) node {$S_2$};
            \draw (9.5,3.5) node {$S_3$};
            \draw (10.5,3.5) node {$S_4$};
            \draw (11.5,3.5) node {$S_5$};

            \draw (7.5,1.5) node {$S_0$};
            \draw (8.5,1.5) node {$S_1$};
            \draw (8.5,1.0) node {$S_6$};
            \draw (9.5,1.5) node {$S_2$};
            \draw (10.5,1.5) node {$S_3$};
            \draw (10.5,1.0) node {$S_4$};
            \draw (11.5,1.5) node {$S_5$};


            % Arrows
            \path[-] (8,-.1) edge [bend left] (6,-1);
            \path[->] (6,-1) edge [bend right] (4,-2);
            \path[->] (10,-.1) edge [bend right] (9,-2);

            % Candidate squares
            \path[fill=green!50] (3.1, -4.0) rectangle (4.9, -2.2);
            \path[fill=red!30] (8.1, -4.0) rectangle (9.9, -2.2);
            \draw (3,-4.1) rectangle (5,-2.1);
            \draw (8,-4.1) rectangle (10,-2.1);

            % Candidate signatures
            \draw (3.5,-2.5) node {$S_0$};
            \draw (3.5,-3.5) node {$S_1$};
            \draw (4.5,-2.5) node {$S_3$};
            \draw (4.5,-3.5) node {$S_4$};
            \draw (4.0,-3) node {$S_6$};

            \draw (9.5,-3) node {$S_5$};
            \draw (8.5,-3) node {$S_2$};

            \draw (9,-4.5) node {Non-candidate signatures};
            \draw (4,-4.5) node {Candidate signatures};
            

        \end{tikzpicture}
    \caption{LSH buckets and bands}
    \label{fig:lsh_buckets}
    \end{center}
\end{figure}

\subsection{Analysis}
In the worst case example, we would get all pairs to be candidate pairs, and the time complexity would be $O(n^2)$ in LSH. But to get to LSH, we would have to do MinHashing first, to get the signature matrix, and MinHashing has a time complexity of $O(n^2k)$. Hopefully, we can discard many signatures, so we only have a fraction as candidate pairs. By looking at the tuning parameters $b$ and $r$, we can calculate the probability $s$ that two signatures will become a candidate pair. \\

In some problems, it is important to have no false negatives, or no false positives. In these problems the threshold can be chosen lower or higher, to match the wanted ratio. We can plot the probability of a particular pair becoming a candidate pair by using the S-curve equation, see figure \ref{fig:scurve} for the plot. In this project we would rather like false positives than false negatives. To achieve a higher amount of false positives, we could adjust our threshold to be higher than the point of the steepest curve. This would mean a higher change for signatures to become candidate pairs. Though in this project's implementation we have sacrificed correctness for speed, so we have chosen our threshold according to \ref{eq:s-estimate}. \\

\begin{equation}
    \text {Probability} = 1 - (1 - s^r)^b 
    \label{eq:s}
\end{equation}\\

\begin{figure}[!htpb]
    \begin{center}
        \input{plots/scurve.tex}
        \caption{S-curve}
        \label{fig:scurve}
    \end{center}
\end{figure}

When choosing $t$ according to equation \ref{eq:s-estimate} - which is what we do in this project - the probability (equation \ref{eq:s}) of becoming a candidate pair is around $0.63$, no matter what $b$ and $r$ is set to. \\

For every band, two signatures have about $1 - (\left(1-s^r \right)$ chance of being regarded as similar, which is not necessarily much. But there are $b$ bands in which this is applicable, which raises the chance of the two whole signatures becoming candidates.

\begin{equation}
    s = (1/b)^{1/r}
    \label{eq:s-estimate}
\end{equation}


\section{B-bit}
B-bit min hashing is a way to estimate set similarity in a more storage efficient way than with min hashing alone. It is a modification for min hashing. The time complexity remains $O(n^2k)$, but out of every signature, only the last few bits are saved. This simulates a low level hashing function, where dissimilar documents could be hashed to the same b-bit value. \\

In our project we found difficulties getting \emph{Python} to store a bit value using less space than an integer below $2^{30}-1 = 1,073,741,823$, or just about 1 billion\footnote{All our objects (integers) would in Python have a size of 8 bytes, not counting the overhead.}. This meant we could not gain much in space complexity using python, since our dataset is only in the scale of millions of distinct object. We will have to cluster the binary values from every signature together in a single integer, and do bitwise operations on that. See \cite{book:bbit} section 5.1  \\

When we are able to store one object using only 1 bit, we only use as many bits, as we have objects. If we have $n$ objects, and they all originally have a minimum size of $s = 8 \cdot 8 = 64$ bits, we would at most use $1/s = 1/64$ of the original space. Notice that the time complexity is still the same. \\

Say we had some hardware that could do boolean comparisons faster than integer comparisons, then a b-bit implementation could utilize this functionality. But that is out of scope of this course. \\


\subsection{The Variance-Space tradeoff}
The variance in our project is a measure of how accurate our results are. We can store about 64 times as many samples when converting to 1 bit, but we lose some accuracy. This is the tradeoff, and can be precisely quantified by the storage factor, which calculates the ratio of the  storage. See equation \ref{storagefactor}.

\begin{equation}
    \text {Storage factor} = B(b;R,r_1,r_2)
    \label{eq:storagefactor}
\end{equation}

Where:\\
$b = $ the number of bits used to store an object. \\
$R = $ the resemblance between two objects \\
$r_1 = $ percentage of total elements in $S_1$ \\
$r_2 = $ percentage of total elements in $S_2$ \\

In \cite{book:bbit} section 2.3, it is deduced that the improvements from using 64 bits to using 1 bit will at least be 21.3-fold. Let us state that $b_1 = 64$ and $b_2 = 1$, then the improvement ratio between the two will be

\begin{equation}
    \frac{B(64;R,r_1,r_2)}{B(1;R,r_1,r_2} \Rightarrow \frac{64 R}{R + 1 - r_1}
    \label{eq:improvementratio}
\end{equation}

In the best case, $R = r_1 = r_2 = 1$, and the improvement ratio will be $\frac{64}{1}$. In the worst case, the resemblance, $R=.5$, and $r_1, r_2 \rightarrow 0$ the ratio will be $\frac{64}{3} \sim 21.3$. See figure \ref{fig:bbit} to see a graph from \cite{book:bbit} which shows the storage improvements with different parameters. \\


\begin{figure}[!htpb]
    \begin{center}
        \includegraphics{plots/bbit/bbit.eps}
        \caption{B-bit storage improvement}
        \label{fig:bbit}
    \end{center}
\end{figure}


\subsection{Running Time}
Our implementation of the b-bit algorithm does not succesfully run on the imdb dataset, only on our dummy data. Part of the reasom for this is that we have not been able to implement a data type with a small enough overhead. So instead of our data actually taking up less space, it takes up the same space, and furthermore additional processing is done to it. See figure \ref{fig:bbit_at}\\

\begin{figure}[!htpb]
    \begin{center}
        \input{plots/bbit_at.tex}
        \caption{B-bit actual running time}
        \label{fig:bbit_at}
    \end{center}
\end{figure}


\section{Odd sketch}

\section{Conclusion}
\newpage
\bibliography{bibliography}
\end{document}





